---
title: |
  | Final Project
  | DS 805: Statistical Learning
author: |
  | Dikshant Joshi, Naveen Kumar Krishnasamy, Rakesh Kumar Nethi
output: html_document
---

## Data Requirements:

- You can pick any data you want as long as it is a classification problem.
- Some sources are:

    - Kaggle <https://www.kaggle.com/datasets?tags=13302-Classification>
    - UCI Machine Learning Repository <https://archive.ics.uci.edu/ml/datasets.php?format=&task=cla&att=&area=&numAtt=&numIns=&type=&sort=nameUp&view=table>
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(MASS)
library(forecast) 
library(ggplot2)
library(ggfortify)
library(kableExtra)
library(caret)
library(class)
library(dplyr)
library(formatR)
library(rpart)
library(rpart.plot)
library(regclass)
library(caret)
library(glmnet)
```

- Read your data in R and call it df. For the rest of this document `y` refers to the variable you are predicting.

```{r}
df = read.table("C:/Users/HP/Downloads/Train.csv", header=TRUE, sep=",")
head(df)
df<-df[-1]
colnames(df)[11]<-"Delivered_Ontime"
```

## The grading rubric can be found below:

+----------------+---------------+--------------------+-----------------------+
|                | R code        | Decision/Why       | Communication         |
|                |               |                    |  of findings          |
+================+===============+====================+=======================+
| Percentage of  | 30%           | 35%                | 35%                   |
| Assigned Points|               |                    |                       |
+----------------+---------------+--------------------+-----------------------+


- **Decision/why?**: Explain your reasoning behind your choice of the procedure, set of variables and such for the question. 

    - Explain why you use the procedure/model/variable
    - To exceed this criterion, describe steps taken to implement the procedure in a non technical way.


- **Communication of your findings**: Explain your results in terms of training MSE, testing MSE, and prediction of the variable `Y` 

    - Explain why you think one model is better than the other.
    - To exceed this criterion, explain your model and how it predicts `y` in a non technical way.


## Part 1: Exploratory Data Analysis (20 points)

1. Check for existence of NA's (missing data)

```{r}
newdata_df<-df[complete.cases(df),]

nrow(df) == nrow(df[complete.cases(df),])
c(nrow(newdata_df),nrow(df))

sum(is.na(df))
```
#Yes, there is some existence of the NA's in our dataset and we 

2. If necessary, classify all categorical variables **except the one you are predicting** as factors. Calculate the summary statistics of the entire data set. 

```{r}
summary(newdata_df)
str(newdata_df)
```


```{r}
newdata_df<-df
newdata_df$Delivered_Ontime[df$Delivered_Ontime=="1"]<-"Yes"
newdata_df$Delivered_Ontime[df$Delivered_Ontime=="0"]<-"No"
df$Delivered_Ontime<-as.factor(df$Delivered_Ontime)
```

3. For the numerical variables, plot box plots based on values of `y`. Do you see a difference between the box plots for any of the variables you choose?

```{r}
#BoxPlot
boxplot(Discount_offered~Delivered_Ontime,data=newdata_df, col='lightblue')
boxplot(Cost_of_the_Product~Delivered_Ontime,data=newdata_df, col='lightblue')
boxplot(Prior_purchases~Delivered_Ontime,data=newdata_df, col='lightblue')
boxplot(Weight_in_gms~Delivered_Ontime,data=newdata_df, col='lightblue')

# donut chart
newdata_df%>%
  group_by(Delivered_Ontime)%>%
  summarise(Count=n())%>%
  mutate(Delivered_Ontime=as.factor(Delivered_Ontime),percentage=round(Count/sum(Count)*100,2),ymax =cumsum(percentage),ymin = c(0, head(ymax, n=-1)),LabelPosition = (ymax + ymin)/2, label = paste0(Delivered_Ontime, "\n value: ", Count, "\n", percentage,"%"))%>%
           ggplot(aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=Delivered_Ontime)) +
  geom_rect() +
  geom_label(x=4, aes(y=LabelPosition, label=label), size=4) +
  scale_fill_brewer(palette=4) +
  coord_polar(theta="y") +
  xlim(c(2, 4)) +
  theme_void()+
  labs(title = "Delivered Ontime (Yes/No)", x = "X-axis Label", y = "Y-axis Label")+
  theme(legend.position = "none")

#density plot
ggplot(newdata_df, aes(x = Weight_in_gms, fill = Delivered_Ontime)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Weight Distribution by Delivery Stat1us")
```
Yes, we observe that the boxplots of delivery status with different variables are different. For the Discount offered variable, we could see that the highly discounted products are seems to be delivered on time and almost all late delivered products are discounted less.
But when we look at the cost of the products that were delivered, both on-time and late delivered shipments have same upper and lower quartile of cost.
When we look at the weight of the products, we could see that late delivered shipments had more weight (average weight of late delivered is closer to 5000gm while on time delivered where near to 3000gm)


4. For the categorical variables, plot bar charts for the different values of `y`. Do you see a difference between plots for any of the variables you choose?

```{r}
#Barplots 
library(dplyr)
total_counts <- newdata_df %>%
  group_by(Warehouse_block) %>%
  summarize(TotalCount = n())

# Calculate percentages within each combination of Warehouse_block and Delivered_Ontime
df_percent <- newdata_df %>%
  group_by(Warehouse_block, Delivered_Ontime) %>%
  summarize(Count = n()) %>%
  left_join(total_counts, by = "Warehouse_block") %>%
  mutate(Percent = Count / TotalCount * 100)

# Create a clustered bar plot with percentage labels
ggplot(df_percent, aes(x = Warehouse_block, y = Count, fill = Delivered_Ontime)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.1f%%", Percent),
                y = Count),  # Adjust the y-coordinate for text placement
            position = position_dodge(width = 0.9),
            vjust = -0.5,
            size = 3) +  # Adjust the size of the text as needed
  labs(title = "Delivery Status by Warehouse Block",
       x = "Warehouse Blocks",
       y = "Total Deliveries") +
  theme_minimal()


#newdata_df$Warehouse_block <- as.factor(df$Warehouse_block)
#ggplot(df, aes(x =  Warehouse_block, fill = Delivered_Ontime)) +
#  labs(title = "Delivery Status by Mode of Shipment", x="Warehouse Blocks", y="# Deliveries")+
#  geom_bar(position=position_dodge())


total_counts <- newdata_df %>%
  group_by(Mode_of_Shipment) %>%
  summarize(TotalCount = n())

# Calculate percentages within each combination of Warehouse_block and Delivered_Ontime
df_percent <- newdata_df %>%
  group_by(Mode_of_Shipment, Delivered_Ontime) %>%
  summarize(Count = n()) %>%
  left_join(total_counts, by = "Mode_of_Shipment") %>%
  mutate(Percent = Count / TotalCount * 100)

# Create a clustered bar plot with percentage labels
ggplot(df_percent, aes(x = Mode_of_Shipment, y = Count, fill = Delivered_Ontime)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.1f%%", Percent),
                y = Count),  # Adjust the y-coordinate for text placement
            position = position_dodge(width = 0.9),
            vjust = -0.5,
            size = 3) +  # Adjust the size of the text as needed
  labs(title = "Delivery Status by Mode of Shipment",
       x = "Mode of Shipment",
       y = "Total Deliveries") +
  theme_minimal()

#newdata_df$Warehouse_block <- as.factor(df$Mode_of_Shipment)
#ggplot(df, aes(x =  Mode_of_Shipment, fill = Delivered_Ontime)) +
#  labs(title = "Delivery Status by Mode of Shipment", x="Mode of Shipment", y="# Deliveries")+
#  geom_bar(position=position_dodge())


ggplot(newdata_df, aes(x = Customer_care_calls , fill = Delivered_Ontime)) +
  labs(title = "Delivery Status by customer care calls", x="# Customer care calls ", y="# Deliveries")+
  geom_bar(position=position_dodge())


ggplot(newdata_df, aes(x = Customer_rating , fill = Delivered_Ontime)) +
  labs(title = "Delivery Status by Customer ratings", x="Customer ratings ", y="# Deliveries")+
  geom_bar(position=position_dodge())

total_counts <- newdata_df %>%
  group_by(Product_importance) %>%
  summarize(TotalCount = n())

# Calculate percentages within each combination of Warehouse_block and Delivered_Ontime
df_percent <- newdata_df %>%
  group_by(Product_importance, Delivered_Ontime) %>%
  summarize(Count = n()) %>%
  left_join(total_counts, by = "Product_importance") %>%
  mutate(Percent = Count / TotalCount * 100)

# Create a clustered bar plot with percentage labels
ggplot(df_percent, aes(x = Product_importance, y = Count, fill = Delivered_Ontime)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.1f%%", Percent),
                y = Count),  # Adjust the y-coordinate for text placement
            position = position_dodge(width = 0.9),
            vjust = -0.5,
            size = 3) +  # Adjust the size of the text as needed
  labs(title = "Delivery Status by Product importance",
       x = "Mode of Shipment",
       y = "Total Deliveries") +
  theme_minimal()


#ggplot(newdata_df, aes(x = Product_importance , fill = Delivered_Ontime)) +
#  labs(title = "Delivery Status by Product Importance", x="Product Priority ", y="# Deliveries")+
#  geom_bar(position=position_dodge())

ggplot(df, aes(x = Gender , fill = Delivered_Ontime)) +
  labs(title = "Delivery Status by Gender", x="Gender ", y="# Deliveries")+
  geom_bar(position=position_dodge())

```
Bar charts also shows the difference in delivery status by the variables. When we see mode of shipment, we could see that most shipments were carried by ship and on-time delivery is higher than late deliveries.
When we observe the delivery status by priority, how and medium prioritized shipments seem to have higher late deliveries (more than 50% of on-time) when compared to high prioritized shipments. Delivery status by customer ratings doesn't seem to provide much information as the highest rated and lowest rated deliveries seems to have same proportion of on-time and late deliveries, implies some other factors could have affected the customer ratings.


6. Test/training separation: Separate your data into 80% training and 20% testing data. Do not forget to set seed. Please use the same separation for the whole assignment, as it is needed to be able to compare the models.


```{r}
## 70% of the sample size for training set
df$Delivered_Ontime=as.factor(df$Delivered_Ontime)

## set the seed to make your partition reproducible
set.seed(123)
index=createDataPartition(df$Delivered_Ontime,p=0.7,list=FALSE)
train_df=df[index,]
test_df=df[-index,]

nrow(train_df)
nrow(test_df)
```



## Part 2: Logistic Regression or LDA (15 points)

1. Develop a classification model where the variable `y` is the dependent variable using the Logistic Regression or LDA, rest of the variables, and your training data set.

```{r}
#Logistic Regression
temp=train_df[,-8]
logfit<-glm(Delivered_Ontime~., data=temp, family=binomial)
summary(logfit)

#y=knn.train[,"Delivered_Ontime"]
#x=model.matrix(Delivered_Ontime~., knn.train)

#lasso.cv=cv.glmnet(x, y,family='binomial', alpha=0)
#Lasso=glmnet(x,y,family='binomial', alpha=0,lambda = lasso.cv$lambda.min)

#coef(Lasso)
```


2.  Obtain the confusion matrix and compute the **testing error rate** based on the logistic regression classification.

```{r}
temp_2=test_df[,-8]
logprob<- predict(logfit, newdata = test_df, type = "response")
head(logprob,3)

logpred=rep(0, nrow(test_df))
logpred[logprob>=.5]=1
logpred=as.factor(logpred)
head(logpred,3)

#confusion Matrix
cm=confusionMatrix(data=logpred, reference=test_df$Delivered_Ontime)
cm

```

```{r}
#testing error
round( mean(logpred!=test_df[,"Delivered_Ontime"]),4)
```
3. Explain your choices and communicate your results.


## Part 3: KNN (15 points)

1. Apply a KNN classification to the training data using.

```{r}
head(train_df,5)

train_knn<-train_df
test_knn<-test_df

#converting categorical variables in factors of numeric for KNN (train_knn & test_knn)

######## Train dataset for Knn
#Gender: M=0, F=1
train_knn[,"Gender"]=ifelse(train_knn[,"Gender"] == "M", 0, 1)
train_knn[,"Gender"]=as.factor(train_knn[,"Gender"])

#Warehouse Block: A=1, B=2, C=3, D=4, E=5, F=6
train_knn[,"Warehouse_block"]=ifelse(train_knn[,"Warehouse_block"] == "A", 1, ifelse(train_knn[,"Warehouse_block"] == "B",2, ifelse( train_knn[,"Warehouse_block"] == "C",3,ifelse(train_knn[,"Warehouse_block"] == "D",4,ifelse(train_knn[,"Warehouse_block"] == "E",5,6)))))
train_knn[,"Warehouse_block"]=as.factor(train_knn[,"Warehouse_block"])


#Mode of Shipment: Ship=1, Road=2, Flight=3
train_knn[,"Mode_of_Shipment"]=ifelse(train_knn[,"Mode_of_Shipment"] == "Ship", 1, ifelse(train_knn[,"Mode_of_Shipment"] == "Road",2,3))
train_knn[,"Mode_of_Shipment"]=as.factor(train_knn[,"Mode_of_Shipment"])

#Product_importance: low=1, medium=2, high=3
train_knn[,"Product_importance"]=ifelse(train_knn[,"Product_importance"] == "low", 1, ifelse(train_knn[,"Product_importance"] == "medium",2,3))                                         
train_knn[,"Product_importance"]=as.factor(train_knn[,"Product_importance"])

train_knn[,"Customer_rating"]=as.factor(train_knn[,"Customer_rating"])



######## Test dataset for Knn
#Gender: M=0, F=1
test_knn[,"Gender"]=ifelse(test_knn[,"Gender"] == "M", 0, 1)
test_knn[,"Gender"]=as.factor(test_knn[,"Gender"])

#Warehouse Block: A=1, B=2, C=3, D=4, E=5, F=6
test_knn[,"Warehouse_block"]=ifelse(test_knn[,"Warehouse_block"] == "A", 1, ifelse(test_knn[,"Warehouse_block"] == "B",2, ifelse( test_knn[,"Warehouse_block"] == "C",3,ifelse(test_knn[,"Warehouse_block"] == "D",4,ifelse(test_knn[,"Warehouse_block"] == "E",5,6)))))
test_knn[,"Warehouse_block"]=as.factor(test_knn[,"Warehouse_block"])

#Mode of Shipment: Ship=1, Road=2, Flight=3
test_knn[,"Mode_of_Shipment"]=ifelse(test_knn[,"Mode_of_Shipment"] == "Ship", 1, ifelse(test_knn[,"Mode_of_Shipment"] == "Road",2,3))
test_knn[,"Mode_of_Shipment"]=as.factor(test_knn[,"Mode_of_Shipment"])

#Product_importance: low=1, medium=2, high=3
test_knn[,"Product_importance"]=ifelse(test_knn[,"Product_importance"] == "low", 1, ifelse(test_knn[,"Product_importance"] == "medium",2,3))                                         
test_knn[,"Product_importance"]=as.factor(test_knn[,"Product_importance"])

test_knn[,"Customer_rating"]=as.factor(test_knn[,"Customer_rating"])

```

```{r}
knn.train=train_knn[,1:11]
knn.test=test_knn[,1:11]
knn.trainLabels=train_knn[,"Delivered_Ontime"]
knn.testLabels=test_knn[,"Delivered_Ontime"]

#KNN model with k=12
knn1 <- knn(train = knn.train, test = knn.test, cl = knn.trainLabels, k=12)

par(mfrow=c(2,2))
library(ggvis)
train_knn %>% ggvis(~Discount_offered, ~Cost_of_the_Product, fill = ~factor(Delivered_Ontime)) %>% layer_points()
train_knn %>% ggvis(~Cost_of_the_Product, ~Weight_in_gms, fill = ~factor(Delivered_Ontime)) %>% layer_points()

```

```{r}
plot(knn1)
```

2.  Obtain the confusion matrix and compute the testing error rate based on the KNN classification.
```{r}
#confusion Matrix
confusionMatrix(data=as.factor(knn1), reference=as.factor(knn.testLabels))

1-mean(knn1==knn.testLabels)
```
KNN with k value 12 was picked randomly on trial & error basis and found to give better accuracy 66.36 (lover error=33.63%) 

```{r}
#train_knn$Delivered_Ontime<-as.numeric(train_knn$Delivered_Ontime)
#test_knn$Delivered_Ontime<-as.numeric(test_knn$Delivered_Ontime)

#train_knn[,"Customer_rating"]=as.numeric(train_knn[,"Customer_rating"])
#train_knn[,"Product_importance"]=as.numeric(train_knn[,"Product_importance"])
#train_knn[,"Mode_of_Shipment"]=as.numeric(train_knn[,"Mode_of_Shipment"])
#train_knn[,"Warehouse_block"]=as.numeric(train_knn[,"Warehouse_block"])
#train_knn[,"Gender"]=as.numeric(train_knn[,"Gender"])


#test_knn[,"Customer_rating"]=as.numeric(test_knn[,"Customer_rating"])
#test_knn[,"Product_importance"]=as.numeric(test_knn[,"Product_importance"])
#test_knn[,"Mode_of_Shipment"]=as.numeric(test_knn[,"Mode_of_Shipment"])
#test_knn[,"Warehouse_block"]=as.numeric(test_knn[,"Warehouse_block"])
#test_knn[,"Gender"]=as.numeric(test_knn[,"Gender"])


set.seed(1994)
k.grid=1:100
error=rep(0, length(k.grid))
knn.train=train_knn[,1:11]
knn.test=test_knn[,1:11]
for (i in seq_along(k.grid)) {
  pred = knn(train = knn.train, 
             test  = knn.test, 
             cl    = knn.trainLabels, 
             k     = k.grid[i])
  error[i] = mean(knn.testLabels !=pred)
}

min(error)
which.min(error)
confusionMatrix(data=as.factor(pred), reference=as.factor(knn.testLabels))
```

```{r}
plot(error, type = "b", col = "dodgerblue", cex = 1, pch = 20, 
     xlab = "k, number of neighbors", ylab = "classification error")
# add line for min error seen
abline(h = min(error), col = "darkorange", lty = 3)

```

3. Explain your choices and communicate your results.


## Part 4: Tree Based Model (15 points)

1. Apply one of the following models to your training data: *Classification Tree, Random Forrest, Bagging or Boosting*
```{r}
#Classification Tree
library(rpart)
library(rpart.plot)
model.ct = rpart(Delivered_Ontime~., train_df, method="class", parms=list(split="gini"))
model.ct
rpart.plot(model.ct,roundint = FALSE)
```

```{r}
#Prediction and confusion matrix
tree_pred=predict(model.ct, newdata=test_df, type="class")
library(caret)
#Confusion matrix
confusionMatrix(
    factor(tree_pred, levels = 0:1),
    factor(test_knn$Delivered_Ontime, levels = 0:1)
)

#testing error rate
round( mean(tree_pred!=test_df[,"Delivered_Ontime"]),4)

#training accuracy
y_train_pred <- predict(model.ct, train_df, type = "class")
confusionMatrix(
  data=factor(y_train_pred),
  reference=factor(train_df$Delivered_Ontime)
)

```

```{r}
#Pruned Tree
#optimum cp
cp_opt <- model.ct$cptable[which.min(model.ct$cptable[, "xerror"]), "CP"]

# Prune the model
model.ct_opt <- prune(tree = model.ct, cp = cp_opt)
                          
# Plot the optimized model
rpart.plot(x = model.ct_opt)
model_pred=predict(model.ct_opt, newdata=test_df, type="class")
confusionMatrix(
    data=factor(model_pred, levels = 0:1),
    reference=factor(test_df$Delivered_Ontime, levels = 0:1)
)

round( mean(model_pred!=test_df[,"Delivered_Ontime"]),4)
```

```{r}
#random Forest
library(randomForest)
set.seed(1234)
fit.forest <- randomForest(Delivered_Ontime ~., data = train_df, importance=TRUE,proximity=TRUE,
                           ntree=500, mtry=2,nodesize=10)

fit.forest

# grid search Hyperparameter tuning for RF
param_grid <- expand.grid(
  ntree = c(200,300,400),
  mtry = c(2, 4, 6),
  nodesize = c(5, 10, 15)
)

# Initialize an empty list to store results
results <- list()

# Perform grid search
for (i in 1:nrow(param_grid)) {
  # Train a Random Forest model using the current set of hyperparameters
  rf_model <- randomForest(
    formula = Delivered_Ontime ~ .,
    data = train_df,
    ntree = param_grid$ntree[i],
    mtry = param_grid$mtry[i],
    nodesize = param_grid$nodesize[i]
  )
  
  # Make predictions on the test set
  predictions <- predict(rf_model, newdata = test_df)
  
  # Evaluate the model's performance (replace "accuracy" with your chosen metric)
  accuracy <- mean(predictions == test_df$Delivered_Ontime)
  
  # Store the results
  results[[i]] <- list(
    ntree = param_grid$ntree[i],
    mtry = param_grid$mtry[i],
    nodesize = param_grid$nodesize[i],
    accuracy = accuracy
  )
}

# Identify the best hyperparameter set
best_params <- results[[which.max(sapply(results, function(x) x$accuracy))]]

# Print the best hyperparameter values
print("Best Hyperparameters:")
print(best_params)
```       

```{r}
head(fit.forest$importance,3)

#OOB error matrix
err= fit.forest$err.rate


head(err)

```

2. Obtain the confusion matrix and compute the testing error rate based on your chosen tree based model.

```{r}
pred.rf= predict(fit.forest, newdata = test_df, type = "class")
# Calculate the confusion matrix for the test set
confusionMatrix(data = pred.rf, reference = test_df$Delivered_Ontime)
```
We performed Random Forest model and the model gave us 65.09% accuracy for the prediction made.

3. Explain your choices and communicate your results.


## Part 5: SVM (15 points)

1. Apply a XG-BOOST model to your training data.

```{r}
#XGBOOST Model
library(xgboost)

#data prep
factor_columns <- sapply(train_knn, is.factor)
d=train_knn
e=test_knn
# Convert factor columns to numeric
d[, factor_columns] <- lapply(d[, factor_columns], function(x) as.integer(x))
d$Gender=d$Gender-1
d$Delivered_Ontime=d$Delivered_Ontime-1
e[, factor_columns] <- lapply(e[, factor_columns], function(x) as.integer(x))
e$Gender=e$Gender-1
e$Delivered_Ontime=e$Delivered_Ontime-1

xgboost_train=xgb.DMatrix(data=as.matrix(d[,-11]), label=d$Delivered_Ontime)
xgboost_test=xgb.DMatrix(data=as.matrix(e[,-11]), label=e$Delivered_Ontime)

#Applying model
param <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  eta = 0.1,
  max_depth = 3,
  alpha = 7,    # L1 regularization term
  gamma = 1
)

xgb_model_f <- xgboost(params = param, 
                     data = xgboost_train, 
                     nrounds = 1000,
                     early_stopping_rounds = 10)

#Prediction
pred_test = predict(xgb_model_f, xgboost_test)
y_test=e[,11]
y_test=as.factor(y_test)
pred_xgb = as.factor(round(pred_test))

conf_mat = confusionMatrix(data=pred_xgb, reference=y_test)
conf_mat

#Prediction
pred_train = predict(xgb_model_f, xgboost_train)
y_train=d[,11]
y_train=as.factor(y_train)
pred_xgb = as.factor(round(pred_train))
conf_mat = confusionMatrix(data=pred_xgb, reference=y_train)
conf_mat
```

3. Explain your choices and communicate your results.

```{r}
library(pROC)
library(ROCR)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(pred_xgb), plot = TRUE, print.auc = TRUE)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(knn1), plot = TRUE, print.auc = TRUE)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(tree_pred), plot = TRUE, print.auc = TRUE)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(pred), plot = TRUE, print.auc = TRUE)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(logpred), plot = TRUE, print.auc = TRUE)
roc.test = roc(as.numeric(test_df$Delivered_Ontime) ~ as.numeric(pred.rf), plot = TRUE, print.auc = TRUE)
# List of predictions
pred.list=list(logpred, knn1, pred, as.numeric(tree_pred), as.numeric(pred_xgb),pred.rf)

# List of actual values
nmod=length(pred.list)
actual=rep(list(test_df$Delivered_Ontime), nmod)
# Plot the ROC curves
library(ROCR)
pred.r=prediction(pred.list, actual)
roc.r=performance(pred.r, "tpr", "fpr")
plot(roc.r, col = as.list(1:nmod), main = "ROC Curves: Test Set", legacy.axes=TRUE, print.auc=TRUE)
legend(x = "bottomright", 
       legend = c("Logistic Regression", "KNN k=12", "KNN K=54", "Classification Tree", "XG-Boost","Random Forest"),
       fill = 1:nmod)

```
